{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Review : Doubly Stochastic Variational Inference for Deep Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Gaussian Process (DGP), a hierarchical composition of Gaussian Processes(GP), can overcome the limitations of standard (single-layer) GP while maintaining the benefits of GP<sup>[1]</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\require{cancel}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard (Single-layer) Gaussian Processes  \n",
    "\n",
    ">Consider inferring a stochastic function $f:\\mathbb{R}^{D}\\to\\mathbb{R}$ , given:\n",
    "- A likelihood $p(y|f)$, models how observations $y$ relate to (latent) function values $f$.\n",
    "- A dataset of $N$ observed outputs $\\mathbf{y}=(y_{1},\\dots,y_{N})^{\\top}$ at input (design) locations $\\mathbf X=(\\mathbf x_{1},\\dots,\\mathbf x_{N})^{\\top}$. \n",
    "\n",
    "> Place a GP prior on function $f$, meaning that any finite collection of function values follows a joint Gaussian distribution, with:\n",
    "- A mean function $m:\\mathbb{R}^{D}\\rightarrow\\mathbb{R}$.\n",
    "- A covariance function (kernel) $k:\\mathbb{R}^{D}\\times\\mathbb{R}^{D}\\overset{.}{\\to}\\mathbb{R}$ encodes similarities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Inducing points** for sparse approximation:\n",
    "\n",
    "1. Define an additional set of $M$ inducing locations $\\mathbf{Z}=(\\mathbf{z}_{1}, \\cdots,\\mathbf{z}_{M})^{\\top}$.\n",
    "\n",
    "2. Write:\n",
    "- $\\mathbf{f}=f(\\mathbf{X})$, $N$-dimensional vector of function values at the observed locations.\n",
    "- $\\mathbf{u}=f(\\mathbf{Z})$, $M$-dimensional vector of function values at inducing locations.\n",
    "\n",
    "By the GP prior, the joint distribution over $(\\mathbf{f},\\mathbf{u})$ is multivariate Gaussian: \n",
    "$$\n",
    "\\begin{bmatrix} \\mathbf{f} \\\\ \\mathbf{u} \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} m(\\mathbf{X}) \\\\ m(\\mathbf{Z}) \\end{bmatrix}, \\begin{bmatrix} K_{\\mathbf{XX}} & K_{\\mathbf{XZ}} \\\\ K_{\\mathbf{ZX}} & K_{\\mathbf{ZZ}} \\end{bmatrix} \\right) \\tag{1}\n",
    "$$\n",
    "- $K_{\\mathbf{XX}}$: $N \\times N$ matrix with entries $[K_{\\mathbf{XX}}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$.\n",
    "- $K_{\\mathbf{ZZ}}$: $M \\times M$ matrix with entries $[K_{\\mathbf{ZZ}}]_{ij} = k(\\mathbf{z}_i, \\mathbf{z}_j)$.\n",
    "- $K_{\\mathbf{XZ}}$ and $K_{\\mathbf{ZX}} = K_{\\mathbf{XZ}}^{\\top}$ link the observed and inducing locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joint density of $\\mathbf{y},\\mathbf{f}$ and $\\mathbf{u}$ is given by:  \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{y},\\mathbf{f},\\mathbf{u})&= p(\\mathbf{f}, \\mathbf{u}) p(\\mathbf{y}|\\mathbf{f}, \\cancel{\\mathbf{u}}) \\\\\n",
    "&= p(\\mathbf{f}|\\mathbf{u}) p(\\mathbf{u}) \\prod_{i=1}^{N}p(y_{i}|f_{i}) \\\\\n",
    "&=\\underbrace{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})p(\\mathbf{u};\\mathbf{Z})}_{\\mathrm{GP~prior}}\\underbrace{\\prod_{i=1}^{N}p(y_{i}|f_{i})}_{\\mathrm{likelihood}}\n",
    "\\end{aligned} \\tag{2}\n",
    "$$  \n",
    "\n",
    "**Notice** that $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ indicates that the input locations for $\\mathbf{f}$ and $\\mathbf{u}$ are $\\mathbf{X}$ and $\\mathbf{Z}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The prior $p(\\mathbf{u}; \\mathbf{Z}) = \\mathcal{N}(\\mathbf{u} | m(\\mathbf{Z}), k(\\mathbf{Z}, \\mathbf{Z}))$\n",
    "2. the conditional $p(\\mathbf{f} | \\mathbf{u}; \\mathbf{X}, \\mathbf{Z}) = \\mathcal{N}(\\mathbf{f} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma})$, where for $i, j = 1, \\ldots, N$:\n",
    "\n",
    "$$[\\boldsymbol{\\mu}]_i = m(\\mathbf{x}_i) + \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} (\\mathbf{u} - m(\\mathbf{Z})) \\tag{3}$$\n",
    "$$[\\mathbf{\\Sigma}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j) - \\boldsymbol{\\alpha}(\\mathbf{x}_i)^{\\top} k(\\mathbf{Z}, \\mathbf{Z}) \\boldsymbol{\\alpha}(\\mathbf{x}_j) \\tag{4}$$\n",
    "\n",
    "with $\\boldsymbol{\\alpha}(\\mathbf{x}_{i})=k(\\mathbf{Z},\\mathbf{Z})^{-1}k(\\mathbf{Z},\\mathbf{x}_{i})$. Inference is possible in closed form when the likelihood $p(y|f)$ is Gaussian, but the time complexity is $O(N^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational Inference for Sparse GP**\n",
    "\n",
    "To handle **(a)** large datasets ($N \\gg M$) **(b)** non-Gaussian likelihoods simultaneously, we need to seek a variational posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Variational inference aims to approximate the true posterior distribution $p(\\mathbf{f},\\mathbf{u})$ with a simpler distribution $q(\\mathbf{f},\\mathbf{u})$, by minimizing the Kullback-Leibler divergence $\\operatorname{KL}[q||p]$ between the variational posterior $q$ and the true posterior $p$ . Equivalently, this can be achieved by maximizing the a lower bound on the marginal likelihood (a.k.a. evidence):  \n",
    "\n",
    "- Start from the log-marginal likelihood (\"evidence\") of the observations $\\mathbf{y}$:\n",
    "\n",
    "$$\\log p(\\mathbf{y}) = \\log \\int p(\\mathbf{y},\\mathbf{f},\\mathbf{u}) d\\mathbf{f} d\\mathbf{u} = \\log \\int q(\\mathbf{f},\\mathbf{u}) \\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})} d\\mathbf{f} d\\mathbf{u}$$\n",
    "\n",
    "- Apply Jensen's inequality with the expectation taken w.r.t $q(\\mathbf{f},\\mathbf{u})$ ($\\log$ is concave):\n",
    "\n",
    "$$$$\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{y}) \\geq \\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right] = \\mathcal{L} \\tag{5}\n",
    "$$  \n",
    "\n",
    "where $p(\\mathbf{y},\\mathbf{f},\\mathbf{u})$ is given in (2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Choose a variational posterior based on the work of Hensman et al. (2013), **_Gaussian process for Big Data_**:\n",
    "\n",
    "$$\n",
    "q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u}) \\tag{6}\n",
    "$$  \n",
    "\n",
    "where $q(\\mathbf{u})=\\mathcal{N}(\\mathbf{u}|\\mathbf{m},\\mathbf{S})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Since both terms in the variational posterior are Gaussian, marginalizing $\\mathbf{u}$ yields:  \n",
    "\n",
    "$$\n",
    "q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=\\int p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})d\\mathbf{u}=\\mathcal{N}(\\mathbf{f}|\\tilde{\\boldsymbol{\\mu}},\\tilde{\\boldsymbol{\\Sigma}}) \\tag{7}\n",
    "$$  \n",
    "\n",
    "$\\tilde{\\boldsymbol{\\mu}}$ and $\\tilde{\\boldsymbol{\\Sigma}}$ can be expressed as the mean and covariance functions of the inputs, similar to equations (3) and (4). This form is commonly used in Bayesian inference with Gaussian random fields (GRF):\n",
    "\n",
    "$$\n",
    "\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i})=m(\\mathbf{x}_{i})+\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(\\mathbf{m}-m(\\mathbf{Z})) \\tag{8}\n",
    "$$\n",
    "$$\n",
    "\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{j})=k(\\mathbf{x}_{i},\\mathbf{x}_{j})-\\boldsymbol{\\alpha}(\\mathbf{x}_{i})^{\\top}(k(\\mathbf{Z},\\mathbf{Z})-\\mathbf{S})\\boldsymbol{\\alpha}(\\mathbf{x}_{j}) \\tag{9}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**\n",
    "\n",
    "The $f_{i}$ marginals of the variational posterior (7) depend only on the corresponding inputs $\\mathbf{x}_{i}$ . Therefore, the i<sup>th</sup> marginal of $q(\\mathbf{f}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})$ can be written as:  \n",
    "\n",
    "$$\n",
    "q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{X},\\mathbf{Z})=q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i})) \\tag{10}\n",
    "$$ \n",
    "<pre> <code>\n",
    " Observations:      y1         y*         yc\n",
    "                    |          |          |\n",
    "Gaussian field:     f1 -- o -- f* -- o -- fc\n",
    "                    |     |    |     |    |\n",
    "     Inputs:        x1    x2   x*   ...   xc\n",
    "</code> </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the variational posterior (6) the lower bound (5) simplifies considerably since:\n",
    "- The conditionals $p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})$ inside the logarithm cancel\n",
    "- The likelihood expectation requires only the variational marginals\n",
    "\n",
    "Let's start with the evidence lower bound (ELBO) from (5):\n",
    "$$\\begin{aligned}\\mathcal{L}&=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right]\\end{aligned}$$\n",
    "From (6), we have:\n",
    "$$q(\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})q(\\mathbf{u})$$ \n",
    "From (2), we know that:\n",
    "$$p(\\mathbf{y},\\mathbf{f},\\mathbf{u})=p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})$$\n",
    "So the ratio inside the logarithm is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{p(\\mathbf{y}, \\mathbf{f}, \\mathbf{u})}{q(\\mathbf{f}, \\mathbf{u})} &= \\frac{\\cancel{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})}p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{\\cancel{p(\\mathbf{f}|\\mathbf{u};\\mathbf{X},\\mathbf{Z})}q(\\mathbf{u})} \\\\\n",
    "&= \\frac{p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{q(\\mathbf{u})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Substituting back into the ELBO, we obtain:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}&=\\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{y},\\mathbf{f},\\mathbf{u})}{q(\\mathbf{f},\\mathbf{u})}\\right] \\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{f},\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{u};\\mathbf{Z})\\prod_{i=1}^{N}p(y_{i}|f_{i})}{q(\\mathbf{u})}\\right] \\\\\n",
    "&= \\mathbb{E}_{q(\\mathbf{u})}\\left[\\log\\frac{p(\\mathbf{u})}{q(\\mathbf{u})}\\right] + \\mathbb{E}_{q(\\mathbf{f})}\\left[\\log\\prod_{i=1}^{N}p(y_{i}|f_{i})\\right] \\\\\n",
    "&= - \\mathrm{KL}[q(\\mathbf{u})||p(\\mathbf{u})] + \\sum_{i=1}^{N}\\mathbb{E}_{q(\\mathbf{f_i})}\\left[\\log p(y_{i}|f_{i})\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since the expectation of the likelihood only requires the marginal variational distributions for each $f_{i}$. From (10) we know that the marginals are given by:\n",
    "$$q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})=\\mathcal{N}(f_{i}|\\mu_{\\mathbf{m},\\mathbf{Z}}(\\mathbf{x}_{i}),\\Sigma_{\\mathbf{S},\\mathbf{Z}}(\\mathbf{x}_{i},\\mathbf{x}_{i}))$$\n",
    "\n",
    "Thus we arrive at the simplified ELBO:\n",
    "$$\\mathcal{L}=\\sum_{i=1}^{N}\\mathbb{E}_{q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})}[\\log p(y_{i}|f_{i})]-\\mathrm{KL}[q(\\mathbf{u})||p(\\mathbf{u})] \\tag{11}$$\n",
    " \n",
    "\n",
    "The final (univariate) expectation of the log-likelihood ($\\color{red}{\\mathbb{E}_{q(f_{i}|\\mathbf{m},\\mathbf{S};\\mathbf{x}_{i},\\mathbf{Z})}[\\log p(y_{i}|f_{i})]}$) can be computed analytically in some cases:\n",
    "- Gauss-Hermite quadrature, Hensman et al., 2015, **_Scalable Variational Gaussian Process Classification_**;\n",
    "- Monte Carlo sampling, Bonilla et al., 2016 **_Generic Inference in Latent Gaussian Process Models_** and Gal et al., 2015 **_Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data_**. \n",
    "\n",
    "The bound is a sum over the data, an unbiased estimator can be obtained through minibatch subsampling. This makes inference feasible on large datasets. A GP uses this inference method referred as a **sparse Gaussian Process (SGP)**.\n",
    "\n",
    "The variational parameters $(\\mathbf{Z},\\mathbf{m}$ and $\\mathbf{S})$ are optimized by maximizing the lower bound we just derived (11). This maximization is guaranteed to converge, as $\\mathcal{L}$ is a valid lower bound to the marginal likelihood $p(\\mathbf{y}|\\mathbf{X})$. The model parameters (hyperparameters of the kernel and likelihood) can also be learned through the maximization this bound. However, caution is needed, as this approach may introduce bias—the bound is not uniformly tight across all hyperparameter settings.\n",
    "\n",
    "For $D$-dimensional outputs $\\mathbf{y}_{i}\\in\\mathbb{R}^{D}$, define $\\mathbf{Y}$ as the matrix with $i$<sup>th</sup> row containing the $i$<sup>th</sup> observation $\\mathbf{y}_{i}$. Similarly, define $\\mathbf{F}$ and $\\mathbf{U}$. If each output is an independent GP we have the GP prior $\\begin{array}{r}{\\prod_{d=1}^{D}p(\\mathbf{F}_{d}|\\mathbf{U}_{d};\\mathbf{X},\\mathbf{Z})p(\\mathbf{U}_{d};\\mathbf{Z})}\\end{array}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A Deep Gaussian Process (DGP) (Damianou and Lawrence, 2013) defines a prior recursively on vector-valued stochastic functions $F^{1}, F^{2},\\ldots,F^{L}$.\n",
    "\n",
    "- Layer-wise GPs:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; At layer $l$, we have $D^{l}$ scalar functions $\\{F_{d}^{l}\\}_{d=1}^{D^{l}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Each $F_{d}^{l}$ is a GP whose inputs are noisy corruptions of the outputs from the previous layer, $F^{l-1}$.\n",
    "\n",
    "- Conditional independence assumption:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Given the input $F^{l-1}$, the $D^{l}$ outputs at layer $l$ are conditionally independent.\n",
    "\n",
    "- Example:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pick M inducing input locations, $Z^{l-1} = \\{z_m^{l-1} \\in \\mathbb{R}^{D^{l-1}}\\}_{m=1}^M$, then collect the corresponding function values into:\n",
    "$$\n",
    "U^l = \\begin{bmatrix}\n",
    "f_1^l(z_1^{l-1}) & \\cdots & f_{D^l}^l(z_1^{l-1}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "f_1^l(z_M^{l-1}) & \\cdots & f_{D^l}^l(z_M^{l-1})\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{M \\times D^l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The noise between layers is assumed i.i.d. Gaussian. Most presentations of DGPs (see, e.g. Damianou and Lawrence, 2013; Bui et al., 2016) explicitly parameterize the noisy corruptions separately from the outputs of each GP.\n",
    "\n",
    "The authors' method of inference does not require us to parameterize these variables separately. For notational convenience, we therefore absorb the noise into the kernel $k_{n o i s y}(\\mathbf{x}_{i},\\mathbf{x}_{j})=\\bar{k}(\\mathbf{x}_{i},\\mathbf{x}_{j})+\\sigma_{l}^{2}\\delta_{i j}$ , where $\\delta_{i j}$ is the Kronecker delta, and $\\sigma_{l}^{2}$ is the noise variance between layers.\n",
    "\n",
    "We still use inducing locations $\\mathbf{Z}^{l-1}$ and corresponding inducing outputs $\\mathbf{U}^{l}$ at each layer.\n",
    "\n",
    "**Joint model and intractable inference**\n",
    "\n",
    "With $\\mathbf{F}^{0}=\\mathbf{X}$, the full joint density as:\n",
    "\n",
    "$$\n",
    "p({\\mathbf{Y}},\\{{\\mathbf{F}}^{l},{\\mathbf{U}}^{l}\\}_{l=1}^{L})=\\underbrace{\\prod_{i=1}^{N}p({\\mathbf{y}}_{i}|{\\mathbf{f}}_{i}^{L})}_{\\mathrm{likelihood}}\\underbrace{\\prod_{l=1}^{L}p({\\mathbf{F}}^{l}|{\\mathbf{U}}^{l};{\\mathbf{F}}^{l-1},{\\mathbf{Z}}^{l-1})p({\\mathbf{U}}^{l};{\\mathbf{Z}}^{l-1})}_{\\mathrm{DGP~prior}},\n",
    "$$  \n",
    "\n",
    "Inference in this model is intractable, so we rely on variational approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variational schemes comparison**\n",
    "\n",
    "The original DGP presentation (Damianou and Lawrence, 2013) uses a variational posterior that maintains the exact model conditioned on $\\mathbf{U}^{l}$ , but further forces the inputs to each layer to be independent from the outputs of the previous layer. \n",
    "\n",
    "- The noisy corruptions are parameterized separately, and the variational distribution over these variables is a fully factorized Gaussian.\n",
    "\n",
    "- This approach requires $2N(D^{1}+\\cdot\\cdot\\cdot+D^{L-1})$ variational parameters but admits a tractable lower bound on the log marginal likelihood if the kernel is of a particular form.\n",
    "\n",
    "- A further problem of this bound is that the density over the outputs is simply a single layer GP with independent Gaussian inputs. Since the posterior loses all the correlations between layers it cannot express the complexity of the full model and so is likely to underestimate the variance. In practice, we found that optimizing the objective in Damianou and Lawrence (2013) results in layers being ‘turned off’ (the signal to noise ratio tends to zero). \n",
    "\n",
    "In contrast, our posterior retains the full conditional structure of the true model. We sacrifice analytical tractability, but due to the sparse posterior within each layer we can sample the bound using univariate Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference<br>\n",
    "<!-- apa -->\n",
    "[1] Salimbeni, H., & Deisenroth, M. (2017). Doubly stochastic variational inference for deep Gaussian processes. Advances in neural information processing systems, 30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
